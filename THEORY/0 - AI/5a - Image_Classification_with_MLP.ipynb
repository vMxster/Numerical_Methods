{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Classificazione di immagini tramite Multi-Layer Perceptron (MLP)**\n",
        "\n",
        "In questa esercitazione impariamo a risolvere un problema di classificazione utilizzando una semplice rete neurale MLP.\n",
        "\n",
        "I dati di input sono immagini, quindi sono dati ad **alta dimensionalità**, presi dal dataset Euclid (lo trovate su Virtuale).\n",
        "\n",
        "La rete impara (*learn*) **da sola** a capire come risolvere il problema."
      ],
      "metadata": {
        "id": "wFjJxN88-_Xw"
      },
      "id": "wFjJxN88-_Xw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerie\n",
        "Importiamo qui le librerie necessarie alla soluzione:\n",
        "\n",
        "*   `Scikit-Learn (sklearn)`: libreria molto potente e vasta principalmente creata per il **Machine Learning** (con alcuni elementi di Deep Learning, come le reti MLP).\n",
        "*   `OpenCV (cv2)`: libreria utilizzata principalmente per gestire le immagini a livello di codice.\n"
      ],
      "metadata": {
        "id": "FQx0egnB_ZTb"
      },
      "id": "FQx0egnB_ZTb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39427db-e6d8-49d0-a395-6d96344edaba",
      "metadata": {
        "id": "d39427db-e6d8-49d0-a395-6d96344edaba"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from os.path import join\n",
        "from tqdm import tqdm\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funzioni di supporto\n",
        "Qui definiamo le funzioni che utilizzeremo nella nostra soluzione.\n",
        "\n",
        "Le funzioni sono utili per avere un codice più modulare e ordinato."
      ],
      "metadata": {
        "id": "jKOGvTBK_dPb"
      },
      "id": "jKOGvTBK_dPb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_labels()` è una funzione che riceve il nome del dato (`string`) e restituisce la classe - **etichetta** -  (`int`), seguendo questa convenzione:\n",
        "\n",
        "*   Triangolo: 0\n",
        "*   Rettangolo: 1\n",
        "*   Quadrato: 2\n",
        "*   Rombo: 3\n",
        "\n",
        "Esempio: 0_triangle.png → 0"
      ],
      "metadata": {
        "id": "xI3jL4Kd_ksT"
      },
      "id": "xI3jL4Kd_ksT"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(name):\n",
        "    if 'triangle' in name:\n",
        "        return 0\n",
        "    elif 'square' in name:\n",
        "        return 1\n",
        "    elif 'rectangle' in name:\n",
        "        return 2\n",
        "    elif 'rhombus' in name:\n",
        "        return 3\n",
        "    else:\n",
        "        raise NotImplementedError('Not existing class!')"
      ],
      "metadata": {
        "id": "vJitaZqSPjXR"
      },
      "id": "vJitaZqSPjXR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`extract_feature()` è una funzione che, data una lista di immagini, calcola le feature associate. Per noi oggi, queste feature corrispondono al contenuto delle immagini \"srotolato\" con valori compresi tra 0 e 1.\n",
        "\n",
        "Per aprire le immagini utilizziamo la libreria `opencv`. Il contenuto delle immagini viene normalizzato tra 0 e 1.\n",
        "\n",
        "**Tools**:\n",
        "*   `cv2.imread()`: apre un'immagine (0: livelli di grigio, 1: colori)\n",
        "*   `cv2.resize()`: ridimensiona un'immagine\n"
      ],
      "metadata": {
        "id": "iCKF6YtKeCXX"
      },
      "id": "iCKF6YtKeCXX"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images, feat_type, img_size):\n",
        "\n",
        "    labels = []\n",
        "    features = []\n",
        "\n",
        "    for image in tqdm(images):\n",
        "\n",
        "        # open the image\n",
        "        img = cv2.imread(image, 0)\n",
        "\n",
        "        # resize the image\n",
        "        img = cv2.resize(img, (img_size, img_size))\n",
        "\n",
        "        # compute the features\n",
        "        if feat_type == 'img':\n",
        "            img = img / 255.0\n",
        "            feat = np.ravel(img)\n",
        "        else:\n",
        "            raise NotImplementedError('Not implemented feature!')\n",
        "\n",
        "        # append features and labels\n",
        "        features.append(feat)\n",
        "        labels.append(get_labels(image))\n",
        "\n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "IDDhAWA_etlP"
      },
      "id": "IDDhAWA_etlP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Soluzione\n",
        "Carica il file .zip che contiene il dataset *Euclid*.\n",
        "Estrai il contenuto con il comando seguente:"
      ],
      "metadata": {
        "id": "zM7mgZQxBub7"
      },
      "id": "zM7mgZQxBub7"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Euclid_dataset.zip -d /content"
      ],
      "metadata": {
        "id": "-KWjFztWfYlJ"
      },
      "id": "-KWjFztWfYlJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/Euclid_dataset'\n",
        "images = glob(join(dataset_path, '*', '*.png'))\n",
        "print('Images: ', len(images))"
      ],
      "metadata": {
        "id": "F9UfwAokQcMJ"
      },
      "id": "F9UfwAokQcMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora è fondamentale dividere il nostro dataset in train, validation e test.\n",
        "Prima di fare questo è necessario mescolare tutti i dati.\n",
        "\n",
        "**Tools**:\n",
        "-    `np.random.shuffle()`: modifica una sequenza mescolando gli elementi."
      ],
      "metadata": {
        "id": "nL_HkwmsCCQ0"
      },
      "id": "nL_HkwmsCCQ0"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Prima shuffling: {}'.format(images[:10]))\n",
        "np.random.shuffle(images)\n",
        "print('Dopo shuffling: {}'.format(images[:10]))"
      ],
      "metadata": {
        "id": "SXKiO_paQjI4"
      },
      "id": "SXKiO_paQjI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mettiamo il **20% dei dati in training, 20% in validation**, e la rimanente parte **(60%) nel test set**."
      ],
      "metadata": {
        "id": "wKpyTz4HCLcC"
      },
      "id": "wKpyTz4HCLcC"
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = images[:int(0.2*len(images))]\n",
        "valset = images[int(0.2*len(images)):int(0.4*len(images))]\n",
        "testset = images[int(0.4*len(images)):]\n",
        "print('Total: {} splitted in Train: {}, Val: {} and Test: {}'.format(len(images), len(trainset), len(valset), len(testset)))"
      ],
      "metadata": {
        "id": "fipgi_u-QnlP"
      },
      "id": "fipgi_u-QnlP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da questo momento, avremo **tre insiemi di dati**: train, validation e test.\n",
        "\n",
        "Ogni elemento del dataset può appartenere solo a uno di questi, gli insieme quindi sono completamente **disgiunti**."
      ],
      "metadata": {
        "id": "4x4XwcitCwig"
      },
      "id": "4x4XwcitCwig"
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 32\n",
        "feature_type = 'img'\n",
        "\n",
        "train_x, train_y = extract_features(trainset, feature_type, img_size)\n",
        "val_x, val_y = extract_features(valset, feature_type, img_size)\n",
        "test_x, test_y = extract_features(testset, feature_type, img_size)\n",
        "\n",
        "print('Train: {}, Val: {} and Test: {}'.format(len(train_x), len(val_x), len(test_x)))\n",
        "print('Total: {}'.format(len(train_x) + len(val_x) + len(test_x)))"
      ],
      "metadata": {
        "id": "SbKcMlOWQtTn"
      },
      "id": "SbKcMlOWQtTn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classificatore\n",
        "Ora definiamo il nostro classificatore che, come detto, è un MLP.\n",
        "Per ora utilizziamo la configurazione di base."
      ],
      "metadata": {
        "id": "u1My56AGDIXj"
      },
      "id": "u1My56AGDIXj"
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MLPClassifier()"
      ],
      "metadata": {
        "id": "ZF5xpgPfQ36w"
      },
      "id": "ZF5xpgPfQ36w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Ora siamo pronti ad addestare il nostro MLP!\n",
        "\n",
        "Fortunatamente la complessità del training di una rete neurale è interamente gestita da `sklearn` (che è molto semplice da utilizzare)\n",
        "\n",
        "**Tools**:\n",
        "-   `model.fit()`: addestra il modello"
      ],
      "metadata": {
        "id": "RLkeVCUxDcIl"
      },
      "id": "RLkeVCUxDcIl"
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "uaoAX6iwQ5hd"
      },
      "id": "uaoAX6iwQ5hd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validazione\n",
        "\n",
        "E' ora di validare il modello per trovare i parametri più corretti.\n",
        "\n",
        "Possiamo cambiare i parametri o i dati del modello, poi eseguire nuovamente un addestramento, e infine capire se le modifiche hanno portato a un incremento o decremento delle prestazioni.\n",
        "\n",
        "**Tools:**\n",
        "*   `score()`: validazione del modello.\n",
        "\n"
      ],
      "metadata": {
        "id": "eNNuHP0_hHYb"
      },
      "id": "eNNuHP0_hHYb"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Validation accuracy: {:.3f}'.format(clf.score(val_x, val_y)))"
      ],
      "metadata": {
        "id": "SmdAmrAfhKKi"
      },
      "id": "SmdAmrAfhKKi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n",
        "Finalmente possiamo testare il nostro modello, facendogli predire in output le classi!\n",
        "\n",
        "**Tools**:\n",
        "  - `model.predict()`: predice la classe per un dato di input"
      ],
      "metadata": {
        "id": "Tuyy2J6hE9V8"
      },
      "id": "Tuyy2J6hE9V8"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = clf.predict(test_x)\n",
        "print('Predicted {} samples: {}'.format(len(pred_y), pred_y))\n",
        "print('GT {} samples: {}'.format(len(test_y), test_y))"
      ],
      "metadata": {
        "id": "_hDdyRVIQ99P"
      },
      "id": "_hDdyRVIQ99P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Tools**:\n",
        "   * `accuracy_score()`: score di accuratezza della classificazione, date in input le classi predette"
      ],
      "metadata": {
        "id": "gCXwtZzSK9H0"
      },
      "id": "gCXwtZzSK9H0"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Final Accuracy: {:.3f}'.format(accuracy_score(test_y, pred_y)))"
      ],
      "metadata": {
        "id": "ln0j7RcbQ_9k"
      },
      "id": "ln0j7RcbQ_9k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusioni\n",
        "La soluzione implementata purtroppo raggiunge una **accuratezza relativamente bassa** (tieni conto che una soluzione random raggiungerebbe il 25% circa di accuratezza).\n",
        "\n",
        "Questo principalmente non è dovuto solamente al classificatore, ma all'alta dimensionalità dei dati in input.\n",
        "\n",
        "Ci sono quindi tre possibili soluzioni:\n",
        "\n",
        "\n",
        "*   **Modificare i parametri del classificatore**, per guadagnare qualcosa in termini di prestazioni. Documentazione: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "*   **Condensare il contenuto informativo dei dati in input**, utilizzando i cosidetti *feature descriptor* (per esempio gli HOG https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients#:~:text=The%20histogram%20of%20oriented%20gradients,localized%20portions%20of%20an%20image). Questo è uno strumento di Visione Artificiale avanzato che non verrà trattato in questo corso.\n",
        "*   **Utilizzare una Convolutional Neural Network (CNN)** che, come visto a lezione, è uno strumento ottimizzato per le immagini. In particolare, è in grado di estrarre in maniera autonoma le feature dalle immagini (nella parte di *Feature Extractor*).\n",
        "\n",
        "E' importante ricordare che, in presenza di dati a bassa dimensionalità (come i valoro numerici provenienti per esempio da un sistema di domotica), una rete MLP dovrebbe avere ottime performance.\n",
        "\n",
        "Nella prossima esercitazione vedremo come implementare una CNN."
      ],
      "metadata": {
        "id": "FlL4dJFskTbY"
      },
      "id": "FlL4dJFskTbY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}